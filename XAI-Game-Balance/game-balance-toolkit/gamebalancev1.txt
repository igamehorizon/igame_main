Here’s a crisp project brief you can paste to Claude so it has full context before reading the code.

# Project Context (for Claude)

## What we’re building

We’re prototyping a **game-agnostic AI + XAI toolkit** to help with **level balancing** in research settings. The **Version 1** deliverable needs to be simple, explainable, and demo-ready (not production). The toolkit ingests raw gameplay events, produces designer-friendly metrics and predictions, and visualizes insights.

## Core ideas

* **Separate skill vs difficulty early.**
  We estimate **player skill** and **level difficulty** via a lightweight **Elo-style model** where each session is a match (player “wins” if success_flag=1).
* **Keep modeling simple, explainable.**
  A **GradientBoostingClassifier** predicts session success using generic features + Elo signals. We use **SHAP** (if available) for global importance and a summary image.
* **Archetypes without labels.**
  We avoid fragile ground-truth labels for “cautious/greedy/explorer” and instead do **unsupervised KMeans** on behavioral features, then apply heuristic names.

## What V1 does (end-to-end)

1. **Load logs** (JSONL/JSON).
   Columns (for best results):
   `timestamp, session_id, player_id, level_id, event_type{level_start|action|level_end}, decision_time_ms, was_backtracked, success_flag (on level_end), completion_time_ms (optional)`
2. **Aggregate to sessions** with generic features:
   `session_time, attempt_count, action_count, mean_decision_time, backtrack_ratio, success_flag, completion_time_ms`
3. **Elo**: compute `player_elo` and `level_elo`.
4. **Model**: train success classifier; save validation AUC.
5. **Archetypes**: KMeans clustering + heuristic names.
6. **Explainability**: SHAP global importances (+ optional plot).
7. **Outputs**:

   * `summary.json` (data stats, AUC, features used, archetype names)
   * `levels/level_<ID>.json` (predicted_success_rate, archetype mix, top features)
   * `sessions_with_preds.csv` (per-session predictions)
   * `shap_summary.png` (if SHAP installed)

## Repo layout (src layout, CLI module)

```
game-balance-toolkit/
├─ README.md
├─ environment.yml                # conda env
├─ pyproject.toml                 # package + console_script (gbt)
├─ requirements.txt
├─ run.py                         # launcher without install
├─ .vscode/launch.json            # VS Code configs (PYTHONPATH set)
├─ src/gbt/
│  ├─ __init__.py
│  ├─ data.py         # loader + synthetic generator
│  ├─ features.py     # event → session features
│  ├─ elo.py          # player/level Elo
│  ├─ model.py        # classifier + SHAP helpers
│  ├─ archetypes.py   # KMeans + heuristic labels
│  └─ cli.py          # CLI (python -m gbt …) / entry-point
├─ apps/
│  └─ streamlit_app.py            # tiny viewer for insights
└─ examples/
   ├─ tiny_showcase.jsonl         # 6 sessions (hand-crafted)
   └─ synth_demo.jsonl            # ~1k sessions (synthetic)
```

## How to run (conda)

```bash
conda env create -f environment.yml
conda activate gbt
# Option 1 — install
pip install -e .
gbt --input examples/synth_demo.jsonl --output output_synth/
# Option 2 — without install (PYTHONPATH)
export PYTHONPATH=src
python -m gbt --input examples/synth_demo.jsonl --output output_synth/
# Option 3 — launcher
python run.py --input examples/synth_demo.jsonl --output output_synth/
```

## Visualize insights (Streamlit)

```bash
streamlit run apps/streamlit_app.py
# In the UI, set “Results folder” to your output directory (e.g., output_synth)
```

Viewer shows:

* Top-level metrics (sessions/players/levels, AUC)
* **Predicted success by level** (bar)
* **Archetype distribution** (bar)
* **Aggregate top features** from SHAP
* **Global SHAP summary image** (if available)
* Filterable **session table**

## Included datasets

* `examples/tiny_showcase.jsonl` – small hand-crafted (6 sessions) for quick sanity checks.
* `examples/synth_demo.jsonl` – synthetic (~1,000 sessions, 50 players, 12 levels) for richer visuals.

## Assumptions & constraints

* Research prototype; emphasis on clarity and explainability over performance.
* SHAP is optional (plots only if installed).
* Archetype naming is heuristic (post-hoc labels on KMeans clusters).
* Elo is a simple, single-pass update (enough for relative skill/difficulty separation in demos).

## What feedback we want from Claude

* Sanity-check the **feature set** for V1: anything obviously missing for game-agnostic difficulty?
* Alternatives to **Elo** or tweaks (e.g., Bayesian rating, logistic mixed models) that are still demo-simple.
* Suggestions to improve **archetype interpretability** with minimal labels (e.g., rule-based descriptors).
* Lightweight ideas for **counterfactual UI** (e.g., sliders for backtrack_ratio → Δ predicted success).
* Any pitfalls in the **data pipeline** or **explainability** choices that could confuse designers.
* Instructions on how to run the showcase and create results.

That’s the full context. After this brief, Claude should be able to understand the purpose, architecture, and how to run/evaluate the toolkit, and then review or propose improvements to the code.
